plot(fit.rock, label.conf.rows = list(label =FALSE), circle = c( "column"))
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
fit.rock
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE), )
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
fit.rock <- unfolding(ratings, circle = c( "row"))
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "row"))
fit.rock
fit.rock <- unfolding(ratings, circle = c( "none"))
plot(fit.rock, label.conf.rows = list(label =FALSE), )
plot(p, 'Shepard', label.conf.rows = list(label =FALSE))
plot(fit.rock, 'Shepard', label.conf.rows = list(label =FALSE))
plot(fit.rock, "stressplot", label.conf.rows = list(label =FALSE))
plot(fit.rock, 'Shepard')
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 7)
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE) )
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 2)
fit.rock
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 7)
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE) )
plot(fit.rock, 'Shepard')
plot(fit.rock, "stressplot", label.conf.rows = list(label =FALSE))
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 10)
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE) )
plot(fit.rock, 'Shepard')
plot(fit.rock, "stressplot", label.conf.rows = list(label =FALSE))
best <- sort(rowMeans(ratings, na.rm = TRUE))[1:10]
worst <- sort(rowMeans(ratings, na.rm = TRUE), decreasing = TRUE)[1:10]
bestworst <- names(c(best, worst))
text(fit.rock$conf.row[bestworst,], labels = bestworst, cex = 0.8, pos = 3,
col = hcl(0, l = 50))
LawlerD <- sim2diss(Lawler)
fit.lawler <- mds(LawlerD, type = "interval")
jackfit <- jackknife(fit.lawler)
jackfit
plot(jackfit)
?jackknife
jackfit <- jackknife(fit.rock)
#############################
# Ratio unfolding
res <- unfolding(breakfast)
res
breakfast
breakfast$toast
breakfast$cornmuff
## various configuration plots
plot(res)
plot(res, type = "p", pch = 25)
plot(res, type = "p", pch = 25, col.columns = 3,
label.conf.columns = list(label = TRUE, pos = 3, col = 3),
col.rows = 8, label.conf.rows = list(label = TRUE, pos = 3, col = 8))
## Shepard plot
plot(res, "Shepard")
## Stress decomposition chart
plot(res, "stressplot")
res
## Stress decomposition chart
plot(res, "stressplot")
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 3000)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 5000)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "col", omega = 0.1, itmax = 10000)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "matrix", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Shepard plot
plot(res, "Shepard")
plot(res)
plot(res, "stressplot")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000, circle='row')
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000, circle='column')
citation('smacof')
shiny::runApp('/srv/shiny-server/sample-apps/CONOCER')
shiny::runApp('/srv/shiny-server/sample-apps/CONOCER')
runApp('/srv/shiny-server/sample-apps/CONOCER')
runApp('/srv/shiny-server/sample-apps/CONOCER')
gc()
plot(c(0,0))
simula.init <- function(n.mas, n.menos, n.test, d)
{
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
library(MASS)
function(i)
{
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
res <- c(error.MDP, i, d)
names(res) <- c('error.MDP', 'corrida','dimension')
return(res)
}
}
#save.image('data2.Rdata')
############ primera simulacion datos normales
set.seed(0)#semilla fuera de las funciones de simulacion
simula.init <- function(n.mas, n.menos, n.test, d)
{
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
library(MASS)
function(i)
{
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
res <- c(error.MDP, i, d)
names(res) <- c('error.MDP', 'corrida','dimension')
return(res)
}
}
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=1600 )
MASS::mvrnorm()
?MASS::mvrnorm
#save.image('data2.Rdata')
############ primera simulacion datos normales
set.seed(0)#semilla fuera de las funciones de simulacion
simula.init <- function(n.mas, n.menos, n.test, d)
{
library(MASS)
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
function(i)
{
library(MASS)
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
####################evaluacion de RLR
library(glmnet)
RLR <- glmnet(x = as.matrix(train[,1:d]), y = factor(train$label) , family = "binomial")
#plot(RLR)
y.RLR <- predict(RLR, newx = as.matrix(test[, 1:d]), type = "class", s =c(0.01)) #el paper dice que usaron este valor de lambda
e <- table( y.RLR, test$label)
error.RLR <- sum(diag(e))/n.test
res <- c(error.MDP, i, d, error.RLR)
names(res) <- c('MDP', 'corrida','dimension', 'RLR')
gc()
return(res)
}
}
t1 <- Sys.time()
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=100 )
library(parallel)
simulacion.10 <- mcmapply( FUN=simula10, 1:100, mc.cores = 6 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=400 )
library(parallel)
simulacion.10 <- mcmapply( FUN=simula10, 1:100, mc.cores = 6 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=100 )
library(parallel)
simulacion.10 <- mcmapply( FUN=simula10, 1:100, mc.cores = 1 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=100 )
library(parallel)
simulacion.10 <- mapply( FUN=simula10, 1:100 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
#save.image('data2.Rdata')
############ primera simulacion datos normales
set.seed(0)#semilla fuera de las funciones de simulacion
simula.init <- function(n.mas, n.menos, n.test, d)
{
library(MASS)
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
function(i)
{
library(MASS)
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
####################evaluacion de RLR
library(glmnet)
RLR <- glmnet(x = as.matrix(train[,1:d]), y = factor(train$label) , family = "binomial")
#plot(RLR)
y.RLR <- predict(RLR, newx = as.matrix(test[, 1:d]), type = "class", s =c(0.01)) #el paper dice que usaron este valor de lambda
e <- table( y.RLR, test$label)
error.RLR <- sum(diag(e))/n.test
res <- c(error.MDP, i, d, error.RLR)
names(res) <- c('MDP', 'corrida','dimension', 'RLR')
gc()
return(res)
}
}
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=400 )
library(parallel)
simulacion.10 <- mapply( FUN=simula10, 1:100 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=400 )
library(parallel)
simulacion.10 <- mapply( FUN=simula10, 1:100 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
25*100/80
25/32
25/31
25*.8
200*.8
shiny::runApp('MCE/Second/CienciaDeDatos/DWD1')
shiny::runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
20*25
setwd('/home/fou/Desktop/MCE/Second/Numerico/ProyectoFinal')
library(Rcpp) #libreria para codigo c++
library(RSpectra) #libreria para lanczos
library(RcppEigen) #libreria para codigo c++
library(imager) #libreria para leer imagenes
library(Matrix)
sourceCpp('W_RGB_float.cpp') #compilamos el programa en C++
t1 <- Sys.time() #medimos tiempo de ejecucion
dir()
imagen <- load.image('001.jpg')
dim(imagen)
plot(imagen) #visualizamos la imagen
#############preprosesamiento  interpolacion
imagen2 <- imagen
imagen2 <- resize(im = imagen, size_x = 3001, size_y = 2257, size_z = 1, size_c = 3 )
#imagen2 <- resize_halfXY(imagen)
#imagen2 <- resize_halfXY(imagen2)
#imagen2 <- resize_halfXY(imagen2)
# Estandarizacion RGB canal por canal: aumentar contraste
imagen2 <- imagen2[, , 1, 1:3]
estandariza <- function(canal){
M1 <- imagen2[,,canal]
M1 <- (M1-min(M1))/(max(M1)-min(M1))
return((M1))
}
M <- lapply(1:3, FUN = estandariza)
library(abind)
M <- abind(M[[1]], M[[2]],M[[3]], along = 3)
gray.imagen <- as.cimg(M)
plot(gray.imagen) #imagen normalizada
## definimos parametros
(h <- 94)
(w <- 120 )
porcentaje <- .1
edges <- ((h*w)*(h*w-1)/2)*(porcentaje) #segun el paper se pueden remover hasta el 90% de las aristas deberia de ser ((h*w)*(h*w+1)/2)*.1
edges.sugerido <- edges/(h*w) #promedio de aristas por nodo
cuadrado <- edges.sugerido**.5 # vamos a fijar esta cantidad
cuadrado <- round(cuadrado) +1
sigJ <- 0.05 #ver paper
sigd <- 10#ver paper
r2 <- cuadrado**2
eje.x <- 0:24*120+1
eje.y <- 0:23*94+1
copia <- gray.imagen[,,1,1:3]
class(copia)
dim(copia)
for( paso.x in eje.x)
{
for(paso.y in eje.y)
{
gc()
t1 <- Sys.time()
M1 <-  copia[paso.x:(paso.x+119), paso.y:(paso.y+93), 1:3 ]
dim(M1)
plot(as.cimg(M1))
#scan()
W <- Kernel_RGB(M1, h, w, r2, sigJ, sigd, dimension=3)
W <- as(W, "sparseMatrix")
print('tick1')
remove(M1)
gc()
#isSymmetric(W)
d <- Matrix::colSums(W) #obtenemos suma por columnas
D_medio <- Matrix::.sparseDiagonal(n = h*w, x = d**(-.5) ) #calculamos la matriz D^{-1/2} para el problema de valores propios generalizado
W <- D_medio%*%(Matrix::.sparseDiagonal(n = h*w, x = d ) -W)%*%D_medio
print('tick2')
Z <- eigs_sym(W, k=2, which='LM', sigma = 0) #usamos implicited restarted lanczos
Z$values #visualizamos los dos valores propios mas pequenios
remove(W) #ahorramos RAM
remove(d)
gc()
print('tick3')
#####################
Y1 <- D_medio%*%(Z$vectors[,1 ]) # Usar los vectores propios que se encontraron para segmentar
hist(as.matrix(Y1))
#Y2 <- D_medio%*%Z$vectors[,2]
#hist(as.matrix(Y2))
set.seed(0)
mascara <- Y1>0
mascara <- matrix(mascara, ncol = h, byrow = TRUE)
segmentacion <- mascara
table(segmentacion)
imagen.segmentacion <- as.cimg(round(segmentacion,1))
plot(imagen.segmentacion)
Aplica.Mascara <- function( x){
M1 <- copia[paso.x:(paso.x+119), paso.y:(paso.y+93) ,x]
M1 <- as.matrix(M1)*mascara
copia[paso.x:(paso.x+119), paso.y:(paso.y+93) ,x] <<- M1
return((copia[ , ,x]))
}
copia <- lapply(1:3, FUN = Aplica.Mascara)
library(abind)
copia <- abind(copia , along = 3)
imagen.final <- as.cimg(copia)
plot(imagen.final)
set.seed(0)
#scan()
t1 <- Sys.time()-t1
print(t1)
print('paso x')
print(paso.x)
print('paso y')
print(paso.y)
#scan()
save(copia, file = 'Ncut.RData')
plot(as.cimg(copia))
#foo <- scan()
gc()
}
}
