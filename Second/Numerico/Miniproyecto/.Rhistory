S12.aprox <-  S12 - (correlaciones.canonicas[i]* (A[,i] %*% t(B[, i]))) #con la primera, ATENCION CON EL INDICE DE correlacion del par acnonico
propo.S11 <- sum(diag(S11 -S11.aprox))/sum(diag(S11))
propo.S22 <- sum(diag(S11 -S22.aprox))/sum(diag(S22))
n <- 50 ##supongamos un tamanio de muestra
estadistico <- -n*log(prod(1-correlaciones.canonicas[i]**2))
valor.critico <- pchisq(1-.05, p*q)
ifelse(estadistico > valor.critico, 'Rechaza H0', 'No se rechaza H0')
i <- 1:2 #pares canonicos a usar
S11.aprox <-  S11 - (A[,i] %*% t(A[, i]))
S22.aprox <-  S22 - (B[,i] %*% t(B[, i]))
S12.aprox <-  S12 - (correlaciones.canonicas[i]* (A[,i] %*% t(B[, i]))) #con la primera, ATENCION CON EL INDICE DE correlacion del par acnonico
propo.S11 <- sum(diag(S11 -S11.aprox))/sum(diag(S11))
propo.S22 <- sum(diag(S11 -S22.aprox))/sum(diag(S22))
n <- 50 ##supongamos un tamanio de muestra
estadistico <- -n*log(prod(1-correlaciones.canonicas[i]**2))
valor.critico <- pchisq(1-.05, p*q)
ifelse(estadistico > valor.critico, 'Rechaza H0', 'No se rechaza H0')
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings)
fit.rock
plot(fit.rock, label.conf.rows = list(label = TRUE))
plot(fit.rock, label.conf.rows = list(label =FALSE))
?unfolding
plot(fit.rock, label.conf.rows = list(label =FALSE), circle = c( "column"))
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
fit.rock
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE), )
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "column"))
fit.rock <- unfolding(ratings, circle = c( "row"))
library(smacof)
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "row"))
fit.rock
fit.rock <- unfolding(ratings, circle = c( "none"))
plot(fit.rock, label.conf.rows = list(label =FALSE), )
plot(p, 'Shepard', label.conf.rows = list(label =FALSE))
plot(fit.rock, 'Shepard', label.conf.rows = list(label =FALSE))
plot(fit.rock, "stressplot", label.conf.rows = list(label =FALSE))
plot(fit.rock, 'Shepard')
ratings <- 11-RockHard[,5:18]
rownames(ratings) <- RockHard[,"Band"]
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 7)
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE) )
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 2)
fit.rock
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 7)
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE) )
plot(fit.rock, 'Shepard')
plot(fit.rock, "stressplot", label.conf.rows = list(label =FALSE))
fit.rock <- unfolding(ratings, circle = c( "none"), ndim= 10)
fit.rock
plot(fit.rock, label.conf.rows = list(label =FALSE) )
plot(fit.rock, 'Shepard')
plot(fit.rock, "stressplot", label.conf.rows = list(label =FALSE))
best <- sort(rowMeans(ratings, na.rm = TRUE))[1:10]
worst <- sort(rowMeans(ratings, na.rm = TRUE), decreasing = TRUE)[1:10]
bestworst <- names(c(best, worst))
text(fit.rock$conf.row[bestworst,], labels = bestworst, cex = 0.8, pos = 3,
col = hcl(0, l = 50))
LawlerD <- sim2diss(Lawler)
fit.lawler <- mds(LawlerD, type = "interval")
jackfit <- jackknife(fit.lawler)
jackfit
plot(jackfit)
?jackknife
jackfit <- jackknife(fit.rock)
#############################
# Ratio unfolding
res <- unfolding(breakfast)
res
breakfast
breakfast$toast
breakfast$cornmuff
## various configuration plots
plot(res)
plot(res, type = "p", pch = 25)
plot(res, type = "p", pch = 25, col.columns = 3,
label.conf.columns = list(label = TRUE, pos = 3, col = 3),
col.rows = 8, label.conf.rows = list(label = TRUE, pos = 3, col = 8))
## Shepard plot
plot(res, "Shepard")
## Stress decomposition chart
plot(res, "stressplot")
res
## Stress decomposition chart
plot(res, "stressplot")
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 3000)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 5000)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "col", omega = 0.1, itmax = 10000)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "matrix", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
res
## Shepard plot
plot(res, "Shepard")
plot(res)
## Shepard plot
plot(res, "Shepard")
plot(res)
plot(res, "stressplot")
plot(res)
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000, circle='row')
## Not run:
## Ordinal unfolding row conditional
## Note that ordinal unfolding may need many iterations (several thousands)
res <- unfolding(breakfast, type = "ordinal", conditionality = "row", omega = 0.1, itmax = 10000, circle='column')
citation('smacof')
shiny::runApp('/srv/shiny-server/sample-apps/CONOCER')
shiny::runApp('/srv/shiny-server/sample-apps/CONOCER')
runApp('/srv/shiny-server/sample-apps/CONOCER')
runApp('/srv/shiny-server/sample-apps/CONOCER')
gc()
plot(c(0,0))
simula.init <- function(n.mas, n.menos, n.test, d)
{
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
library(MASS)
function(i)
{
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
res <- c(error.MDP, i, d)
names(res) <- c('error.MDP', 'corrida','dimension')
return(res)
}
}
#save.image('data2.Rdata')
############ primera simulacion datos normales
set.seed(0)#semilla fuera de las funciones de simulacion
simula.init <- function(n.mas, n.menos, n.test, d)
{
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
library(MASS)
function(i)
{
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
res <- c(error.MDP, i, d)
names(res) <- c('error.MDP', 'corrida','dimension')
return(res)
}
}
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=1600 )
MASS::mvrnorm()
?MASS::mvrnorm
#save.image('data2.Rdata')
############ primera simulacion datos normales
set.seed(0)#semilla fuera de las funciones de simulacion
simula.init <- function(n.mas, n.menos, n.test, d)
{
library(MASS)
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
function(i)
{
library(MASS)
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
####################evaluacion de RLR
library(glmnet)
RLR <- glmnet(x = as.matrix(train[,1:d]), y = factor(train$label) , family = "binomial")
#plot(RLR)
y.RLR <- predict(RLR, newx = as.matrix(test[, 1:d]), type = "class", s =c(0.01)) #el paper dice que usaron este valor de lambda
e <- table( y.RLR, test$label)
error.RLR <- sum(diag(e))/n.test
res <- c(error.MDP, i, d, error.RLR)
names(res) <- c('MDP', 'corrida','dimension', 'RLR')
gc()
return(res)
}
}
t1 <- Sys.time()
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=100 )
library(parallel)
simulacion.10 <- mcmapply( FUN=simula10, 1:100, mc.cores = 6 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=400 )
library(parallel)
simulacion.10 <- mcmapply( FUN=simula10, 1:100, mc.cores = 6 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=100 )
library(parallel)
simulacion.10 <- mcmapply( FUN=simula10, 1:100, mc.cores = 1 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=100 )
library(parallel)
simulacion.10 <- mapply( FUN=simula10, 1:100 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
#save.image('data2.Rdata')
############ primera simulacion datos normales
set.seed(0)#semilla fuera de las funciones de simulacion
simula.init <- function(n.mas, n.menos, n.test, d)
{
library(MASS)
#CLOSURE para simular obetner el errror de clasificaci贸n de MDP
#esta funcion regresa una funcion
#fijamos los parametros y la muestra
n.mas <- n.mas
n.menos <- n.menos
n.test <- n.test
d <- d
I <- diag(d)
n <- n.mas + n.menos
pos1 <- mvrnorm(n = n.mas, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
pos1 <- as.data.frame(pos1)
neg1 <- mvrnorm(n = n.menos, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
neg1 <- as.data.frame(neg1)
train <- rbind(pos1,neg1)
train$label <- 1
train$label[(n.mas+1):(2*n.mas)] <- -1
function(i)
{
library(MASS)
#########genracion de conjunto test
test.1 <- mvrnorm(n = n.test/2, mu =c(2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.1 <- as.data.frame(test.1)
test.1$label <- 1
test.2 <- mvrnorm(n = n.test/2, mu =c(-2.2, rep(0, d-1) ), Sigma = diag(rep(1,d)))
test.2 <- as.data.frame(test.2)
test.2$label <- -1
test <- rbind(test.1, test.2)
##############evaluacion de MDP
pos.mean <- apply(pos1, 2, mean)
neg.mean <- apply(neg1, 2, mean)
X <- ginv(cov(train[,  - (d+1)])) %*% (pos.mean - neg.mean)
MDP <- X/sum(X**2)**.5 #normalizamos el vector
test.predic <- as.matrix(test[, 1:d])%*%MDP
test$y_hat <-  ifelse(test.predic>=0, 1, -1)
error.MDP <- 1-sum(diag(table(test$label, test$y_hat)))/n.test
####################evaluacion de RLR
library(glmnet)
RLR <- glmnet(x = as.matrix(train[,1:d]), y = factor(train$label) , family = "binomial")
#plot(RLR)
y.RLR <- predict(RLR, newx = as.matrix(test[, 1:d]), type = "class", s =c(0.01)) #el paper dice que usaron este valor de lambda
e <- table( y.RLR, test$label)
error.RLR <- sum(diag(e))/n.test
res <- c(error.MDP, i, d, error.RLR)
names(res) <- c('MDP', 'corrida','dimension', 'RLR')
gc()
return(res)
}
}
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=400 )
library(parallel)
simulacion.10 <- mapply( FUN=simula10, 1:100 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
t1 <- Sys.time()
simula10 <- simula.init(n.mas = 25, n.menos=25, n.test =200, d=400 )
library(parallel)
simulacion.10 <- mapply( FUN=simula10, 1:100 )
simulacion.10 <- as.data.frame(t(simulacion.10))
t1 <- Sys.time() - t1
t1 #tiempo hasta 1600 solo para MDP con 100
25*100/80
25/32
25/31
25*.8
200*.8
shiny::runApp('MCE/Second/CienciaDeDatos/DWD1')
shiny::runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
runApp('MCE/Second/CienciaDeDatos/DWD1')
20*25
setwd('/home/fou/Desktop/MCE/Second/Numerico/Miniproyecto')
#library(Rcpp) #libreria para codigo c++
#library(RcppEigen) #libreria para codigo c++
#library(RSpectra) #libreria para lanczos
library(imager) #libreria para leer imagenes
#library(rtiff)
#library(abind)
#sourceCpp('W1.cpp') #compilamos el programa en C++
t1 <- Sys.time() #medimos tiempo de ejecucion
dir()
imagen <- load.image(file = 'redbloodcell-100.JPG' )
M <- grayscale(imagen)
plot(M)
dim(M)
w <- dim(M)[1] #ancho de la imagen
h <- dim(M)[2] #altura de la imagen
#standarizacion RGB canal por canal
M <- ((M- min(M))/(max(M)-min(M)))*255
###################### Parametros para construccion de Kernel
porcentaje <- .1 # de momento nos quedamos con el 90%
edges <- ((h*w)*(h*w-1)/2)*(porcentaje) #segun el paper se pueden remover hasta el 90% de las aristas deberia de ser ((h*w)*(h*w+1)/2)*.1
edges.sugerido <- edges/(h*w) #promedio de aristas por nodo
cuadrado <- edges.sugerido**.5 # vamos a fijar esta cantidad
cuadrado <- round(cuadrado) +1
sigJ <- 0.05 #ver paper
sigd <- 10#ver paper
r2 <- cuadrado**2
######################### implicit restarted Lanczos method, IRLM, implementation
#################################################
set.seed(0)
k <- 3 #numero de vectores propios que se quiere
V <- matrix(rep(0, h*w*k), ncol = k) #matriz de vectores propios
T <- matrix(rep(0, h*w*k), ncol = k) #matrix de bandas
beta <- matrix(rep(0, k), ncol= k )
r <- matrix(rep(0, h*w), ncol = 1)
######################## Lanzcos implementation
r <- runif(h*w, -1, 1) #el vector r de la citada implementacion, comenzamos aleatorio
beta[1] <- (sum(r**2))**(.5)
dot <- matrix(rep(0, h*w), ncol = 1)
punto <- matrix(rep(0, h*w), ncol = (h*w))
#standarizacion RGB canal por canal
M <- ((M- min(M))/(max(M)-min(M)))*255
M <- as.matrix(M)
######################## Lanzcos implementation
r <- runif(h*w, -1, 1) #el vector r de la citada implementacion, comenzamos aleatorio
r <- matrix(r, byrow = TRUE, ncol = 1)
r*beta[j]
j <-1
alpha <- matrix(rep(0, k), ncol = k)
r <- matrix(rep(0, h*w), ncol = 1)
########################## se incializa, implentacion de http://www.netlib.org/utk/people/JackDongarra/etemplates/node120.html#fig:ksteplan
reglon.W <- function(i)
{
punto <- matrix(rep(0, h*w), ncol = (h))
# construccion del i-esimo reglon de W
for (l in 1:h)
for(m in 1:w)
{
d2 <- (((i%%h)-l)**2 + ((i%%w)-m)**2)**.5
if(d2 <= r2)
{
punto[l, m] <- (exp(-pow(M[l,m])- M[i%%h, i%%w ],2)/(2*pow(sigJ,2))-d2/(2*pow(sigd,2)))
}
}
punto <- matrix(punto, byrow = TRUE, ncol = h*w)
return(punto)
}
dot.lanzcos <- function(V, imagen,  h, w)
{
# funcion para generar el producto punto de W*v_j, W:kernel de 'imagen' sin tener que almacenar W
gc()
dot <- matrix(rep(0, h*w), ncol = 1)
for (i in 1:(h*w))
{
#para cada pixel
#genramos el i-esimo reglon de W y lo multiplicamos por V
dot[i] <- sum(reglon.W(i, imagen)*V)
}
return(dot)
}
######################## Lanzcos implementation
r <- runif(h*w, -1, 1) #el vector r de la citada implementacion, comenzamos aleatorio
r <- matrix(r, byrow = TRUE, ncol = 1)
beta[1] <- (sum(r**2))**(.5)
rho <- 1
